{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwEhC/9lvlbk0EJ9xjwbbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/necrodancer/Today-I-learned/blob/master/DL_implementation/DenseNet_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNErlhrRooLb",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet PyTorch Implementation\n",
        "* 2020-02-17에 생성됨.\n",
        "* DenseNet의 구조를 이해하고, PyTorch에 친숙해지기 위해 HOYA012님의 구현체를 따라해보는 노트북입니다. 아래 참조링크를 통해 이론과 소스코드를 따라가 보았습니다.\n",
        "* [“DenseNet Tutorial \\[1\\] Paper Review & Implementation details”](https://hoya012.github.io/blog/DenseNet-Tutorial-1/)\n",
        "* [“DenseNet Tutorial \\[2\\] PyTorch Code Implementation”](https://hoya012.github.io/blog/DenseNet-Tutorial-2/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jI4PLxindzE",
        "colab_type": "text"
      },
      "source": [
        "# Preparation Step\n",
        "\n",
        "* torch 관련 패키지, 이미지 관련 패키지 import하기.\n",
        "* 하이퍼파라미터 설정하기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQUs61NnYlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fd8419cf-4175-445b-d5d5-54ff3e15e46e"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from torch.utils import data  as D\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "import torchsummary\n",
        "\n",
        "print(torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-xPpK5QoLLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "validation_ratio = 0.1\n",
        "random_seed = 10\n",
        "initial_lr = 0.1\n",
        "num_epoch = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFsClLxpocNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "8777e9e1-a866-4a7e-fae5-75144d8dfc18"
      },
      "source": [
        "# transforms.Compose 문서: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "transform_train = transforms.Compose([  # compose 함수는 여러 전처리(변환) 과정을 묶어서 수행한다.\n",
        "        #transforms.Resize(32),\n",
        "        transforms.RandomCrop(32, padding=4),  # 랜덤 크롭\n",
        "        transforms.RandomHorizontalFlip(),  # 가로 뒤집기\n",
        "        transforms.ToTensor(),  # 텐서화, 0~255를 0~1로 스케일링하는 기능도 있는듯함(https://mjdeeplearning.tistory.com/81)\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]) # (sequence of means, sequence of stds), x,y,z 채널에 대해 각각 mean값과 std 값을 넣어준 것 z = (x-mean)/std\n",
        "        # 참고로 위 mean, std는 densenet 논문에서 cifar10의 픽셀 평균 및 표준편차를 구한걸 255로 나눈 값이라고 한다.\n",
        "        # 해당 링크 참조: https://github.com/kuangliu/pytorch-cifar/issues/16\n",
        "\n",
        "transform_validation = transforms.Compose([\n",
        "        #transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "        # validation은 말그대로 평가용이니까 크롭이나 플립을 따로 실시하지 않은 듯하다.\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        #transforms.Resize(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(  # datasets.ImageFolder를 사용하면 커스텀 이미지 데이터셋을 사용할 수 있다. https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "validset = torchvision.datasets.CIFAR10(  # 똑같은 train set에 대해 변환만 validation_transform을 써줬군\n",
        "    root='./data', train=True, download=True, transform=transform_validation)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(validation_ratio * num_train))\n",
        "print(split)\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)  # dataset index를 미리 셔플해주는군  \n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)  # 주어진 인덱스 대로 데이터를 랜덤 샘플함.  https://pytorch.org/docs/stable/data.html\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(  # 데이터로더에 데이터셋 object, 배치사이즈, 샘플러가 들어가는군.\n",
        "    trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    validset, batch_size=batch_size, sampler=valid_sampler, num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')  # 10 classes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMG9VT2y-yd",
        "colab_type": "text"
      },
      "source": [
        "참고) Cifar10 데이터셋의 구성:\n",
        "\n",
        "각각의 레이블마다 32×32 크기 이미지인 50,000개의 training 데이터셋, 10,000개의 test 데이터셋이 존재하고, 결과적으로 총 60,000개의 32×32 크기의 이미지로 데이터셋이 구성되어 있다. http://solarisailab.com/archives/2325"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDgXDZPMzPul",
        "colab_type": "text"
      },
      "source": [
        "# Module Class 생성\n",
        "* DenseNet 구성 모듈과 대응되는 클래스를 각각 생성\n",
        "* 각 클래스를 조립하여 전체 구조를 구성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhOedjCAywqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bn_relu_conv class\n",
        "class bn_relu_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
        "        super(bn_relu_conv, self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(nin)\n",
        "        # in-place in python: https://www.tutorialspoint.com/inplace-operator-in-python\n",
        "        # torch.nn.ReLU: https://pytorch.org/docs/stable/nn.html\n",
        "        self.relu = nn.ReLU(True)  # True는 in-place연산, 즉 복사없이 바로 값을 연산해서 저장하는 걸(e.g. a += 1) 허용한다는 뜻. \n",
        "        self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.batch_norm(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4OHFi7x1da1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bottleneck_layer class\n",
        "\"\"\"\n",
        "DenseNet-BC에서 사용되는 layer\n",
        "1x1->3x3\n",
        "drop out 구현\n",
        "torch.cat을 통해 freature map을 channel-wise로 누적\n",
        "\"\"\"\n",
        "class bottleneck_layer(nn.Sequential):  # 위와 달리 Sequential을 상속한다.\n",
        "    def __init__(self, nin, growth_rate, drop_rate=0.2):\n",
        "        super(bottleneck_layer, self).__init__()\n",
        "\n",
        "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))  # 1by1이라서 따로 패딩 안 해줌. growth_rate*4인 이유는 bottle neck 진입시 너무 축약되지 않도록 4를 하이퍼파라미터로써 곱해준 것 같음\n",
        "        self.add_module('conv_3x3', bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))  # 3by3 conv인데 출력 크기를 유지하고 싶으니 패딩 1 해준 듯. 채널 수 뻥튀기 방지를 위해 출력은 그냥 growth_rate만큼의 채널로 나옴\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottleneck_output = super(bottleneck_layer, self).forward(x)  # sequential을 상속 받아서 forward하면 add_module에 있는 module들이 순차적으로 실행되나보군\n",
        "        if self.drop_rate > 0:\n",
        "            bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training) # nn.functional 안에 dropout이 있군\n",
        "            # 변수명을 output 대신 out이라 적어서 끊겨 있었다.....\n",
        "        bottleneck_output = torch.cat((x, bottleneck_output), 1)  # 1번째 채널에 맞춰 concat\n",
        "        return bottleneck_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3MNScghNCrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transition layer class\n",
        "'''\n",
        "1x1 conv -> 2x2 avg pool (feature size and channel reduction)\n",
        "theta로 output feature map의 개수 조절 가능\n",
        "'''\n",
        "class Transition_layer(nn.Sequential):\n",
        "    def __init__(self, nin, theta=0.5):\n",
        "        super(Transition_layer, self).__init__()\n",
        "\n",
        "        self.add_module('conv_1x1', bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
        "        self.add_module('avg_pool_2x2', nn.AvgPool2d(kernel_size=2, stride=2, padding=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwjJKwJhN-kU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dense block class\n",
        "\"\"\"\n",
        "bottleneck layer를 연속적으로 만들어줘서 하나의 dense block을 이루게 한다.\n",
        "잘 보면 nin_bottleneck_layer 변수를 통해 입력 feature map이 등차 수열을 이루는 걸 확인할 수 있다. (https://hoya012.github.io/blog/DenseNet-Tutorial-1/)\n",
        "\"\"\"\n",
        "class DenseBlock(nn.Sequential):\n",
        "    def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
        "        super(DenseBlock, self).__init__()\n",
        "\n",
        "        for i in range(num_bottleneck_layers):\n",
        "            nin_bottleneck_layer = nin + growth_rate * i  # 여기 밑에도 nin 입력을 nin으로 해놔서 잘못된 크기가 들어가고 있었다..\n",
        "            self.add_module('bottleneck_layer_%d'%i,bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4OnKqRPJN6",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet-BC 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCvZDYjcOtXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):  # 논문에서 제시한 파라미터 값이겠지\n",
        "        super(DenseNet, self).__init__()\n",
        "        \n",
        "        assert (num_layers - 4) % 6 == 0  # 왜 6의 배수가 되어야 할까? 아래 답변 참고\n",
        "        \n",
        "        # (num_layers-4)//6 \n",
        "        num_bottleneck_layers = (num_layers - 4) // 6  # 이해했다. densenet-bc-100-12 자체가 총 layer 개수가 100개고, 이 중 4개를 제외한 나머지가 dense block에 사용된다.\n",
        "                                                       # 이때 논문에서 각 dense block마다 사용된 bottleneck layer는 16개였다. 그래서 6으로 나눈 거고. 위 assert는 그런 배수를 지키라는 의미\n",
        "                                                       # https://towardsdatascience.com/densenet-on-cifar10-d5651294a1a8\n",
        "        # 32 x 32 x 3 --> 32 x 32 x (growth_rate*2)\n",
        "        self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "                \n",
        "        # 32 x 32 x (growth_rate*2) --> 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 32 x 32 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)] --> 16 x 16 x [(growth_rate*2) + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_1 = (growth_rate*2) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_1 = Transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
        "        \n",
        "        # 16 x 16 x nin_transition_layer_1*theta --> 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "\n",
        "        # 16 x 16 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)] --> 8 x 8 x [nin_transition_layer_1*theta + (growth_rate * num_bottleneck_layers)]*theta\n",
        "        nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        self.transition_layer_2 = Transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
        "        \n",
        "        # 8 x 8 x nin_transition_layer_2*theta --> 8 x 8 x [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)]\n",
        "        self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "        \n",
        "        nin_fc_layer = int(nin_transition_layer_2*theta) + (growth_rate * num_bottleneck_layers) \n",
        "        \n",
        "        # [nin_transition_layer_2*theta + (growth_rate * num_bottleneck_layers)] --> num_classes\n",
        "        self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        dense_init_output = self.dense_init(x)  # 위는 그냥 object 또는 method를 축약해놓은 거였네. 실제로 인자를 넣고 하는 건 forward에서 이루어지는군.\n",
        "        \n",
        "        dense_block_1_output = self.dense_block_1(dense_init_output)\n",
        "        transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
        "        \n",
        "        dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
        "        transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
        "        \n",
        "        dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
        "        \n",
        "        global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1, 1))   # glabal avg pool도 nn.functional 안에 있구만. 여기서 (1, 1)은 output size 즉 8x8->1x1을 뜻한다.           \n",
        "        global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)  # (1,1)을 펴주는군\n",
        "\n",
        "        output = self.fc_layer(global_avg_pool_output_flat)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh5PbttrRqoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DenseNetBC_100_12():\n",
        "    return DenseNet(growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10)\n",
        "\n",
        "def DenseNetBC_250_24():\n",
        "    return DenseNet(growth_rate=24, num_layers=250, theta=0.5, drop_rate=0.2, num_classes=10)\n",
        "\n",
        "def DenseNetBC_190_40():\n",
        "    return DenseNet(growth_rate=40, num_layers=190, theta=0.5, drop_rate=0.2, num_classes=10)\n",
        "\n",
        "# 이런 파라미터 관리 좋은 습관인 것 같다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk6GyeTLVAgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99c06b83-d5d8-44f2-fc96-efa3997b7e20"
      },
      "source": [
        "net = DenseNetBC_100_12()\n",
        "net.to(device)  # 마지막으로 DenseNet 객체를 생성한 후 이를 .to로 device에 등록해야 한다."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (dense_block_1): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_1): Transition_layer(\n",
              "    (conv_1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_2): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_2): Transition_layer(\n",
              "    (conv_1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool_2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_3): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv_3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc_layer): Linear(in_features=342, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BmN0jgvVT7I",
        "colab_type": "text"
      },
      "source": [
        "# Architecture Summary\n",
        "* torch summary 라이브러리를 통해 자신이 계산한 layer 수와 실제 생성된 모델의 layer 수를 비교할 수 있음\n",
        "* 모든 연산들의 parameter 수, output shape, total parameter 수, 학습에 필요한 memory size 등을 표시해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-gs-j8RVr4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "382acaf3-e688-4b12-8f44-c6e764debe4e"
      },
      "source": [
        "torchsummary.summary(net, (3, 32, 32)) # 모델 object와 입력 사이즈를 인자롤 받는다."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             672\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "              ReLU-3           [-1, 24, 32, 32]               0\n",
            "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
            "      bn_relu_conv-5           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-6           [-1, 48, 32, 32]              96\n",
            "              ReLU-7           [-1, 48, 32, 32]               0\n",
            "            Conv2d-8           [-1, 12, 32, 32]           5,184\n",
            "      bn_relu_conv-9           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-10           [-1, 36, 32, 32]              72\n",
            "             ReLU-11           [-1, 36, 32, 32]               0\n",
            "           Conv2d-12           [-1, 48, 32, 32]           1,728\n",
            "     bn_relu_conv-13           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
            "             ReLU-15           [-1, 48, 32, 32]               0\n",
            "           Conv2d-16           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-17           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
            "             ReLU-19           [-1, 48, 32, 32]               0\n",
            "           Conv2d-20           [-1, 48, 32, 32]           2,304\n",
            "     bn_relu_conv-21           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 48, 32, 32]              96\n",
            "             ReLU-23           [-1, 48, 32, 32]               0\n",
            "           Conv2d-24           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-25           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-26           [-1, 60, 32, 32]             120\n",
            "             ReLU-27           [-1, 60, 32, 32]               0\n",
            "           Conv2d-28           [-1, 48, 32, 32]           2,880\n",
            "     bn_relu_conv-29           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-30           [-1, 48, 32, 32]              96\n",
            "             ReLU-31           [-1, 48, 32, 32]               0\n",
            "           Conv2d-32           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-33           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-34           [-1, 72, 32, 32]             144\n",
            "             ReLU-35           [-1, 72, 32, 32]               0\n",
            "           Conv2d-36           [-1, 48, 32, 32]           3,456\n",
            "     bn_relu_conv-37           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-38           [-1, 48, 32, 32]              96\n",
            "             ReLU-39           [-1, 48, 32, 32]               0\n",
            "           Conv2d-40           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-41           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-42           [-1, 84, 32, 32]             168\n",
            "             ReLU-43           [-1, 84, 32, 32]               0\n",
            "           Conv2d-44           [-1, 48, 32, 32]           4,032\n",
            "     bn_relu_conv-45           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-46           [-1, 48, 32, 32]              96\n",
            "             ReLU-47           [-1, 48, 32, 32]               0\n",
            "           Conv2d-48           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-49           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-50           [-1, 96, 32, 32]             192\n",
            "             ReLU-51           [-1, 96, 32, 32]               0\n",
            "           Conv2d-52           [-1, 48, 32, 32]           4,608\n",
            "     bn_relu_conv-53           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
            "             ReLU-55           [-1, 48, 32, 32]               0\n",
            "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-57           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-58          [-1, 108, 32, 32]             216\n",
            "             ReLU-59          [-1, 108, 32, 32]               0\n",
            "           Conv2d-60           [-1, 48, 32, 32]           5,184\n",
            "     bn_relu_conv-61           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-62           [-1, 48, 32, 32]              96\n",
            "             ReLU-63           [-1, 48, 32, 32]               0\n",
            "           Conv2d-64           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-65           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-66          [-1, 120, 32, 32]             240\n",
            "             ReLU-67          [-1, 120, 32, 32]               0\n",
            "           Conv2d-68           [-1, 48, 32, 32]           5,760\n",
            "     bn_relu_conv-69           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-70           [-1, 48, 32, 32]              96\n",
            "             ReLU-71           [-1, 48, 32, 32]               0\n",
            "           Conv2d-72           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-73           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-74          [-1, 132, 32, 32]             264\n",
            "             ReLU-75          [-1, 132, 32, 32]               0\n",
            "           Conv2d-76           [-1, 48, 32, 32]           6,336\n",
            "     bn_relu_conv-77           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-78           [-1, 48, 32, 32]              96\n",
            "             ReLU-79           [-1, 48, 32, 32]               0\n",
            "           Conv2d-80           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-81           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-82          [-1, 144, 32, 32]             288\n",
            "             ReLU-83          [-1, 144, 32, 32]               0\n",
            "           Conv2d-84           [-1, 48, 32, 32]           6,912\n",
            "     bn_relu_conv-85           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-86           [-1, 48, 32, 32]              96\n",
            "             ReLU-87           [-1, 48, 32, 32]               0\n",
            "           Conv2d-88           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-89           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-90          [-1, 156, 32, 32]             312\n",
            "             ReLU-91          [-1, 156, 32, 32]               0\n",
            "           Conv2d-92           [-1, 48, 32, 32]           7,488\n",
            "     bn_relu_conv-93           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-94           [-1, 48, 32, 32]              96\n",
            "             ReLU-95           [-1, 48, 32, 32]               0\n",
            "           Conv2d-96           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-97           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-98          [-1, 168, 32, 32]             336\n",
            "             ReLU-99          [-1, 168, 32, 32]               0\n",
            "          Conv2d-100           [-1, 48, 32, 32]           8,064\n",
            "    bn_relu_conv-101           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-102           [-1, 48, 32, 32]              96\n",
            "            ReLU-103           [-1, 48, 32, 32]               0\n",
            "          Conv2d-104           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-105           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-106          [-1, 180, 32, 32]             360\n",
            "            ReLU-107          [-1, 180, 32, 32]               0\n",
            "          Conv2d-108           [-1, 48, 32, 32]           8,640\n",
            "    bn_relu_conv-109           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-110           [-1, 48, 32, 32]              96\n",
            "            ReLU-111           [-1, 48, 32, 32]               0\n",
            "          Conv2d-112           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-113           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-114          [-1, 192, 32, 32]             384\n",
            "            ReLU-115          [-1, 192, 32, 32]               0\n",
            "          Conv2d-116           [-1, 48, 32, 32]           9,216\n",
            "    bn_relu_conv-117           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-118           [-1, 48, 32, 32]              96\n",
            "            ReLU-119           [-1, 48, 32, 32]               0\n",
            "          Conv2d-120           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-121           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-122          [-1, 204, 32, 32]             408\n",
            "            ReLU-123          [-1, 204, 32, 32]               0\n",
            "          Conv2d-124           [-1, 48, 32, 32]           9,792\n",
            "    bn_relu_conv-125           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-126           [-1, 48, 32, 32]              96\n",
            "            ReLU-127           [-1, 48, 32, 32]               0\n",
            "          Conv2d-128           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-129           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-130          [-1, 216, 32, 32]             432\n",
            "            ReLU-131          [-1, 216, 32, 32]               0\n",
            "          Conv2d-132          [-1, 108, 32, 32]          23,328\n",
            "    bn_relu_conv-133          [-1, 108, 32, 32]               0\n",
            "       AvgPool2d-134          [-1, 108, 16, 16]               0\n",
            "     BatchNorm2d-135          [-1, 108, 16, 16]             216\n",
            "            ReLU-136          [-1, 108, 16, 16]               0\n",
            "          Conv2d-137           [-1, 48, 16, 16]           5,184\n",
            "    bn_relu_conv-138           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-139           [-1, 48, 16, 16]              96\n",
            "            ReLU-140           [-1, 48, 16, 16]               0\n",
            "          Conv2d-141           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-142           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-143          [-1, 120, 16, 16]             240\n",
            "            ReLU-144          [-1, 120, 16, 16]               0\n",
            "          Conv2d-145           [-1, 48, 16, 16]           5,760\n",
            "    bn_relu_conv-146           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-147           [-1, 48, 16, 16]              96\n",
            "            ReLU-148           [-1, 48, 16, 16]               0\n",
            "          Conv2d-149           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-150           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-151          [-1, 132, 16, 16]             264\n",
            "            ReLU-152          [-1, 132, 16, 16]               0\n",
            "          Conv2d-153           [-1, 48, 16, 16]           6,336\n",
            "    bn_relu_conv-154           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-155           [-1, 48, 16, 16]              96\n",
            "            ReLU-156           [-1, 48, 16, 16]               0\n",
            "          Conv2d-157           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-158           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-159          [-1, 144, 16, 16]             288\n",
            "            ReLU-160          [-1, 144, 16, 16]               0\n",
            "          Conv2d-161           [-1, 48, 16, 16]           6,912\n",
            "    bn_relu_conv-162           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-163           [-1, 48, 16, 16]              96\n",
            "            ReLU-164           [-1, 48, 16, 16]               0\n",
            "          Conv2d-165           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-166           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-167          [-1, 156, 16, 16]             312\n",
            "            ReLU-168          [-1, 156, 16, 16]               0\n",
            "          Conv2d-169           [-1, 48, 16, 16]           7,488\n",
            "    bn_relu_conv-170           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-171           [-1, 48, 16, 16]              96\n",
            "            ReLU-172           [-1, 48, 16, 16]               0\n",
            "          Conv2d-173           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-174           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-175          [-1, 168, 16, 16]             336\n",
            "            ReLU-176          [-1, 168, 16, 16]               0\n",
            "          Conv2d-177           [-1, 48, 16, 16]           8,064\n",
            "    bn_relu_conv-178           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-179           [-1, 48, 16, 16]              96\n",
            "            ReLU-180           [-1, 48, 16, 16]               0\n",
            "          Conv2d-181           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-182           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-183          [-1, 180, 16, 16]             360\n",
            "            ReLU-184          [-1, 180, 16, 16]               0\n",
            "          Conv2d-185           [-1, 48, 16, 16]           8,640\n",
            "    bn_relu_conv-186           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-187           [-1, 48, 16, 16]              96\n",
            "            ReLU-188           [-1, 48, 16, 16]               0\n",
            "          Conv2d-189           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-190           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-191          [-1, 192, 16, 16]             384\n",
            "            ReLU-192          [-1, 192, 16, 16]               0\n",
            "          Conv2d-193           [-1, 48, 16, 16]           9,216\n",
            "    bn_relu_conv-194           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-195           [-1, 48, 16, 16]              96\n",
            "            ReLU-196           [-1, 48, 16, 16]               0\n",
            "          Conv2d-197           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-198           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-199          [-1, 204, 16, 16]             408\n",
            "            ReLU-200          [-1, 204, 16, 16]               0\n",
            "          Conv2d-201           [-1, 48, 16, 16]           9,792\n",
            "    bn_relu_conv-202           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-203           [-1, 48, 16, 16]              96\n",
            "            ReLU-204           [-1, 48, 16, 16]               0\n",
            "          Conv2d-205           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-206           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-207          [-1, 216, 16, 16]             432\n",
            "            ReLU-208          [-1, 216, 16, 16]               0\n",
            "          Conv2d-209           [-1, 48, 16, 16]          10,368\n",
            "    bn_relu_conv-210           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-211           [-1, 48, 16, 16]              96\n",
            "            ReLU-212           [-1, 48, 16, 16]               0\n",
            "          Conv2d-213           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-214           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-215          [-1, 228, 16, 16]             456\n",
            "            ReLU-216          [-1, 228, 16, 16]               0\n",
            "          Conv2d-217           [-1, 48, 16, 16]          10,944\n",
            "    bn_relu_conv-218           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-219           [-1, 48, 16, 16]              96\n",
            "            ReLU-220           [-1, 48, 16, 16]               0\n",
            "          Conv2d-221           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-222           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-223          [-1, 240, 16, 16]             480\n",
            "            ReLU-224          [-1, 240, 16, 16]               0\n",
            "          Conv2d-225           [-1, 48, 16, 16]          11,520\n",
            "    bn_relu_conv-226           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-227           [-1, 48, 16, 16]              96\n",
            "            ReLU-228           [-1, 48, 16, 16]               0\n",
            "          Conv2d-229           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-230           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-231          [-1, 252, 16, 16]             504\n",
            "            ReLU-232          [-1, 252, 16, 16]               0\n",
            "          Conv2d-233           [-1, 48, 16, 16]          12,096\n",
            "    bn_relu_conv-234           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-235           [-1, 48, 16, 16]              96\n",
            "            ReLU-236           [-1, 48, 16, 16]               0\n",
            "          Conv2d-237           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-238           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-239          [-1, 264, 16, 16]             528\n",
            "            ReLU-240          [-1, 264, 16, 16]               0\n",
            "          Conv2d-241           [-1, 48, 16, 16]          12,672\n",
            "    bn_relu_conv-242           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-243           [-1, 48, 16, 16]              96\n",
            "            ReLU-244           [-1, 48, 16, 16]               0\n",
            "          Conv2d-245           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-246           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-247          [-1, 276, 16, 16]             552\n",
            "            ReLU-248          [-1, 276, 16, 16]               0\n",
            "          Conv2d-249           [-1, 48, 16, 16]          13,248\n",
            "    bn_relu_conv-250           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-251           [-1, 48, 16, 16]              96\n",
            "            ReLU-252           [-1, 48, 16, 16]               0\n",
            "          Conv2d-253           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-254           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-255          [-1, 288, 16, 16]             576\n",
            "            ReLU-256          [-1, 288, 16, 16]               0\n",
            "          Conv2d-257           [-1, 48, 16, 16]          13,824\n",
            "    bn_relu_conv-258           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-259           [-1, 48, 16, 16]              96\n",
            "            ReLU-260           [-1, 48, 16, 16]               0\n",
            "          Conv2d-261           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-262           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-263          [-1, 300, 16, 16]             600\n",
            "            ReLU-264          [-1, 300, 16, 16]               0\n",
            "          Conv2d-265          [-1, 150, 16, 16]          45,000\n",
            "    bn_relu_conv-266          [-1, 150, 16, 16]               0\n",
            "       AvgPool2d-267            [-1, 150, 8, 8]               0\n",
            "     BatchNorm2d-268            [-1, 150, 8, 8]             300\n",
            "            ReLU-269            [-1, 150, 8, 8]               0\n",
            "          Conv2d-270             [-1, 48, 8, 8]           7,200\n",
            "    bn_relu_conv-271             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
            "            ReLU-273             [-1, 48, 8, 8]               0\n",
            "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-275             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-276            [-1, 162, 8, 8]             324\n",
            "            ReLU-277            [-1, 162, 8, 8]               0\n",
            "          Conv2d-278             [-1, 48, 8, 8]           7,776\n",
            "    bn_relu_conv-279             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-280             [-1, 48, 8, 8]              96\n",
            "            ReLU-281             [-1, 48, 8, 8]               0\n",
            "          Conv2d-282             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-283             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-284            [-1, 174, 8, 8]             348\n",
            "            ReLU-285            [-1, 174, 8, 8]               0\n",
            "          Conv2d-286             [-1, 48, 8, 8]           8,352\n",
            "    bn_relu_conv-287             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-288             [-1, 48, 8, 8]              96\n",
            "            ReLU-289             [-1, 48, 8, 8]               0\n",
            "          Conv2d-290             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-291             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-292            [-1, 186, 8, 8]             372\n",
            "            ReLU-293            [-1, 186, 8, 8]               0\n",
            "          Conv2d-294             [-1, 48, 8, 8]           8,928\n",
            "    bn_relu_conv-295             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-296             [-1, 48, 8, 8]              96\n",
            "            ReLU-297             [-1, 48, 8, 8]               0\n",
            "          Conv2d-298             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-299             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-300            [-1, 198, 8, 8]             396\n",
            "            ReLU-301            [-1, 198, 8, 8]               0\n",
            "          Conv2d-302             [-1, 48, 8, 8]           9,504\n",
            "    bn_relu_conv-303             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-304             [-1, 48, 8, 8]              96\n",
            "            ReLU-305             [-1, 48, 8, 8]               0\n",
            "          Conv2d-306             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-307             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-308            [-1, 210, 8, 8]             420\n",
            "            ReLU-309            [-1, 210, 8, 8]               0\n",
            "          Conv2d-310             [-1, 48, 8, 8]          10,080\n",
            "    bn_relu_conv-311             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-312             [-1, 48, 8, 8]              96\n",
            "            ReLU-313             [-1, 48, 8, 8]               0\n",
            "          Conv2d-314             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-315             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-316            [-1, 222, 8, 8]             444\n",
            "            ReLU-317            [-1, 222, 8, 8]               0\n",
            "          Conv2d-318             [-1, 48, 8, 8]          10,656\n",
            "    bn_relu_conv-319             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-320             [-1, 48, 8, 8]              96\n",
            "            ReLU-321             [-1, 48, 8, 8]               0\n",
            "          Conv2d-322             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-323             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-324            [-1, 234, 8, 8]             468\n",
            "            ReLU-325            [-1, 234, 8, 8]               0\n",
            "          Conv2d-326             [-1, 48, 8, 8]          11,232\n",
            "    bn_relu_conv-327             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-328             [-1, 48, 8, 8]              96\n",
            "            ReLU-329             [-1, 48, 8, 8]               0\n",
            "          Conv2d-330             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-331             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-332            [-1, 246, 8, 8]             492\n",
            "            ReLU-333            [-1, 246, 8, 8]               0\n",
            "          Conv2d-334             [-1, 48, 8, 8]          11,808\n",
            "    bn_relu_conv-335             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-336             [-1, 48, 8, 8]              96\n",
            "            ReLU-337             [-1, 48, 8, 8]               0\n",
            "          Conv2d-338             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-339             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-340            [-1, 258, 8, 8]             516\n",
            "            ReLU-341            [-1, 258, 8, 8]               0\n",
            "          Conv2d-342             [-1, 48, 8, 8]          12,384\n",
            "    bn_relu_conv-343             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-344             [-1, 48, 8, 8]              96\n",
            "            ReLU-345             [-1, 48, 8, 8]               0\n",
            "          Conv2d-346             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-347             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-348            [-1, 270, 8, 8]             540\n",
            "            ReLU-349            [-1, 270, 8, 8]               0\n",
            "          Conv2d-350             [-1, 48, 8, 8]          12,960\n",
            "    bn_relu_conv-351             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-352             [-1, 48, 8, 8]              96\n",
            "            ReLU-353             [-1, 48, 8, 8]               0\n",
            "          Conv2d-354             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-355             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-356            [-1, 282, 8, 8]             564\n",
            "            ReLU-357            [-1, 282, 8, 8]               0\n",
            "          Conv2d-358             [-1, 48, 8, 8]          13,536\n",
            "    bn_relu_conv-359             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-360             [-1, 48, 8, 8]              96\n",
            "            ReLU-361             [-1, 48, 8, 8]               0\n",
            "          Conv2d-362             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-363             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-364            [-1, 294, 8, 8]             588\n",
            "            ReLU-365            [-1, 294, 8, 8]               0\n",
            "          Conv2d-366             [-1, 48, 8, 8]          14,112\n",
            "    bn_relu_conv-367             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-368             [-1, 48, 8, 8]              96\n",
            "            ReLU-369             [-1, 48, 8, 8]               0\n",
            "          Conv2d-370             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-371             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-372            [-1, 306, 8, 8]             612\n",
            "            ReLU-373            [-1, 306, 8, 8]               0\n",
            "          Conv2d-374             [-1, 48, 8, 8]          14,688\n",
            "    bn_relu_conv-375             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-376             [-1, 48, 8, 8]              96\n",
            "            ReLU-377             [-1, 48, 8, 8]               0\n",
            "          Conv2d-378             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-379             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-380            [-1, 318, 8, 8]             636\n",
            "            ReLU-381            [-1, 318, 8, 8]               0\n",
            "          Conv2d-382             [-1, 48, 8, 8]          15,264\n",
            "    bn_relu_conv-383             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-384             [-1, 48, 8, 8]              96\n",
            "            ReLU-385             [-1, 48, 8, 8]               0\n",
            "          Conv2d-386             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-387             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-388            [-1, 330, 8, 8]             660\n",
            "            ReLU-389            [-1, 330, 8, 8]               0\n",
            "          Conv2d-390             [-1, 48, 8, 8]          15,840\n",
            "    bn_relu_conv-391             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-392             [-1, 48, 8, 8]              96\n",
            "            ReLU-393             [-1, 48, 8, 8]               0\n",
            "          Conv2d-394             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-395             [-1, 12, 8, 8]               0\n",
            "          Linear-396                   [-1, 10]           3,430\n",
            "================================================================\n",
            "Total params: 768,502\n",
            "Trainable params: 768,502\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 87.35\n",
            "Params size (MB): 2.93\n",
            "Estimated Total Size (MB): 90.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fN380Scciz8",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrHGGzF_VyS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "691f5450-c439-499e-ab7d-a4f50901b176"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()  # loss를 criterion이라고 표현하는구나\n",
        "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch * 0.5), int(num_epoch * 0.75)], gamma=0.1, last_epoch=-1)\n",
        "               # last_epoch: The index of last epoch. Default: -1. (https://pytorch.org/docs/stable/optim.html)\n",
        "for epoch in range(num_epoch):  \n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):  # enumerate(sequence, start=0) (https://docs.python.org/ko/3/library/functions.html)\n",
        "        inputs, labels = data  # 배치 사이즈 만큼 데이터가 나온다. (https://seobway.tistory.com/entry/torchutilsdataDataLoader-%EA%B0%9C%EC%9D%B8-%EC%A0%95%EB%A6%AC)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clears the gradients of all optimized torch.Tensor s.\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        show_period = 100\n",
        "        if i % show_period == show_period-1:    # print every \"show_period\" mini-batches\n",
        "            print('[%d, %5d/50000] loss: %.7f' %\n",
        "                  (epoch + 1, (i + 1)*batch_size, running_loss / show_period))\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        \n",
        "    # validation part\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(valid_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)  # Returns a namedtuple (values, indices)  (https://pytorch.org/docs/stable/torch.html#torch.max)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('[%d epoch] Accuracy of the network on the validation images: %d %%' % \n",
        "          (epoch + 1, 100 * correct / total)\n",
        "         )\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  6400/50000] loss: 2.2754670\n",
            "[1, 12800/50000] loss: 2.1744711\n",
            "[1, 19200/50000] loss: 2.0452563\n",
            "[1, 25600/50000] loss: 1.9625419\n",
            "[1, 32000/50000] loss: 1.8791807\n",
            "[1, 38400/50000] loss: 1.8255380\n",
            "[1, 44800/50000] loss: 1.7602180\n",
            "[1 epoch] Accuracy of the network on the validation images: 35 %\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xaOtwSoguIn",
        "colab_type": "text"
      },
      "source": [
        "# Test\n",
        "* Test set에 대해 test를 한 뒤 10가지 클래스마다 정확도를 각각 구하고, 또한 전체 정확도를 구하는 과정이 위에 코드로 구현이 되어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIKkBSubfDLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "b3a3c823-1b43-4082-bc4f-e84b34dd560f"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # for test\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "                \n",
        "        for i in range(labels.shape[0]):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))            \n",
        "            \n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i])) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 34 %\n",
            "Accuracy of plane : 32 %\n",
            "Accuracy of   car : 59 %\n",
            "Accuracy of  bird : 15 %\n",
            "Accuracy of   cat : 28 %\n",
            "Accuracy of  deer : 32 %\n",
            "Accuracy of   dog : 22 %\n",
            "Accuracy of  frog : 40 %\n",
            "Accuracy of horse : 28 %\n",
            "Accuracy of  ship : 61 %\n",
            "Accuracy of truck : 28 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiRo5wBmhhrG",
        "colab_type": "text"
      },
      "source": [
        "# 후기\n",
        "1. PyTorch에서 제공하는 DataLoader로 전처리를 가공하는 흐름을 알 수 있었다. 추가적으로 커스텀 데이터를 적용하는 법도 링크를 달아놨다.\n",
        "2. bnreluconv 모듈부터 bottleneck layer, dense block, 마지막으로 densenet-bc-100-12에 이르는 모델 구현과정을 확인할 수 있었다.\n",
        "3. 타이핑 중에 변수명이나 파라미터를 잘못 입력하여 네트워크 입출력이 이상하게 나왔다. 그래서 torch summary에서 자꾸 오류가 났고, 출력된 densenet의 입출력과 에러 로그를 대조하면서 대략 2, 3 군데 잘못 입력된 부분을 확인하고 수정했다.\n",
        "4. 모델 학습 및 평가 코드에 대해서도 살펴볼 수 있었다. 필요한 내용은 그때그때 문서를 참고하고 링크를 달아두었다.\n",
        "5. torch summary에 모델의 메모리 용량까지 표시해주는 점이 인상 깊었다.\n",
        "6. 오래 걸릴까봐 일부러 1epoch만 돌렸는데 생각보다 빨리 끝나서 놀랐다. cifar 이미지 크기가 작아서 그렇다고 생각한다. 오랜만에 모델 학습 과정을 지켜보니 감회가 새로웠다.\n",
        "7. 따라 치다가 복붙 후 주석을 다는 식으로 공부방법을 변경했는데 속도 면에서는 좋았으나 기억에 얼마나 남을지는 모르겠다. 주요 부분은 여전히 타이핑을 해야겠다는 생각이 들었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9K7DkcchFHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}