{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3분 딥러닝 3장 실습\n",
    "\n",
    "# 20171130 목\n",
    "p.35~38\n",
    "\n",
    "작성된 소스 코드의 출처 : https://github.com/golbin/TensorFlow-Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant의 반환값은 Tensor()라고 해서 자료형과 속성에 대해 설명한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1., 2., 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1., 2., 3.], [4., 5., 6.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 텐서플로에서 수학식 계산을 위한 가장 기본적인 자료형.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랭크 = '\\['의 갯수, 즉 차원 수. \n",
    "(예) 3 : 랭크 0, [3]: 랭크 1, [[3]]: 랭크 2, [[[3]]]: 랭크 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랭크 0부터 순서대로, 스칼라, 벡터, 행렬, ... n차원 텐서라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셰이프는 랭크 별 원소 개수.\n",
    "(예) 3 : 랭크 0 셰이프 \\[\\], \\[3\\] : 랭크 1 셰이프 \\[1\\], [[[1., 2., 3.]],[[1., 2., 3.]]] : 랭크 3 셰이프 [2, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 프로그램 구조 : 1. 그래프 생성(동그라미, tf.constant, tf.add, ...) 2. 그래프 실행(Session run). 이런 실행 방식을 '지연 실행' 또는 'Lazy Evaluation'이라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 실행은 세션 안에서 이루어져야 한다. 아래는 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리 : 텐서플로우는 그래프 기반 머신러닝 라이브러리이다. 프로그램 실행은 그래프 생성, 그래프 실행 순으로 진행된다. 랭크는 \\[의 개수이며 차원 수를 의미한다. 랭크 0부터 스칼라, 벡터, 행렬, ... n차원 텐서라고 부른다. 셰이프는 각 랭크 별 원소 수를 뜻한다. [제일 큰 랭크의 원소수, 그 다음 랭크의 원소 수, ... ]로 적힌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171206 수\n",
    "<br>\n",
    "p.38~43\n",
    "<br>\n",
    "플레이스홀더와 변수\n",
    "플레이스홀더 : 그래프에 입력 값을 담을 그릇(parameter)을 담당.\n",
    "변수(variable) : 학습 함수들이 학습 결과를 갱신하기 위해 사용하는 변수(가중치)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None : 크기가 정해져 있지 않다.\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,3],[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.random_normal은 정규 분포를 따르는 무작위 값으로 초기화한다는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2,3]행렬 = 행이 2, 열이 3\n",
    "<br>\n",
    "(예) [[1,2,3],[1,2,3]] : [2, 3] 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholder가 session에서 돌아가려면 feed_dict를 통해 입력 데이터를 별도로 넣어야 에러가 나지 않는다.\n",
    "<br>\n",
    "애초에 placeholder는 그릇이니까 내용물을 feed_dict로 담는 건 자연스러운 일이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171207 목\n",
    "\n",
    "p. 44~\n",
    "<br>\n",
    "선형 회귀 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X,Y -> 관계(직선 또는 곡선)\n",
    "<br>\n",
    "X -관계(직선 또는 곡선)-> Y\n",
    "<br>\n",
    "머신러닝은 이렇게 X, Y의 관계를 찾아내서\n",
    "<br>\n",
    "새로운 X를 넣었을 때 Y를 예측하는 것을 기본으로 한다.\n",
    "<br>\n",
    "<br>\n",
    "균등 분포란 주사위를 던지는 확률이 모두 1/6이듯이 사건이 일어날 확률이 모두 동일한 확률 분포를 뜻한다.\n",
    "<br>\n",
    "손실 함수, loss function은 한 쌍의 데이터(x,y)에 대한 손실값을 계산하는 함수이다.\n",
    "<br>\n",
    "손실값이란 예측 값과 실제 값 사이의 차이를 뜻한다. 이 차이를 최소화할 수 있다면 예측도는 최대가 될 것이다.\n",
    "<br>\n",
    "손실값은 각 데이터에 대한 오차를 뜻하고, 비용(Cost)는 전체 데이터에 대한 손실을 뜻한다. 즉 각 손실값 등을 평균하는 것 등으로 구할 수 있다.\n",
    "<br>\n",
    "학습이란 W, b와 같은 변수에 다양한 값을 넣고 계산해서 손실값을 계산하고, 이 손실을 최소화하는 W, b를 구하는 과정이다.\n",
    "<br>\n",
    "손실 함수 중 대표적인 것이 예측 값 - 실제 값이다. 즉 차이, 거리를 뜻한다.\n",
    "<br>\n",
    "최소제곱 함수라고 하여서 이 차이에다가 제곱을 씌운 손실 함수를 아래 코드에 사용했다.\n",
    "<br>\n",
    "그리고 비용을 계산하기 위해 각 손실값들을 모두 더한 후 평균을 내는 reduce_mean을 사용했다. 여기서 reduce란 여러 개를 더해서 한 개의 숫자로 결과가 나오기 때문에 붙인 말로 보인다.\n",
    "<br>\n",
    "그렇다면 어떻게 비용을 최소화해야할까? 최적화 함수란 걸 쓰면 된다. 가장 유명한 게 경사하강법, Gradient Descent이다. 이는 손실함수의 미분값(변화량)을 변수에 반영(학습률을 곱해서 통제) 반영해서 손실값을 줄이는 새로운 변수를 갱신하는 방식을 택한다. 따라서 최적화함수를 많이 작동시키면 그 만큼 변수가 갱신될 것이고, 손실값도 점점 줄어들 것이다. (오버피팅은 일단 논외로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "hypothesis = W * X + b # W와 X까 헁렬이 아니므로 matmul을 사용하지 않음.\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data,\n",
    "                                                           Y: y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\",sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5 Y:\",sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171208금\n",
    "p.52~<br>\n",
    "# Ch.4 기본 신경망 구현\n",
    "(이어서 해야 함)\n",
    "# 20171210 일\n",
    "(이어서 계속)\n",
    "p.62 ~<br>\n",
    "인공 신경망은 뉴런의 구조에서 영감을 받은 구조로 가중치+활성화 함수의 연결로 구성되어 있다.<br>\n",
    "가장 좋은 결과를 내기 위해 가중치를 일일이 찾을 수고를 제한된 볼츠만, 각종 활성화 함수, 역전파를 통해 찾을 수 있게 되었고, <br>\n",
    "데이터 수의 기하급수 증가, GPU에 의한 컴퓨팅 파워 증가로 인해 신경망 학습은 딥러닝이라는 이름 아래 최고의 호황을 누리고 있다. <br>\n",
    "역전파는 결과값과 실제값의 오차를 각 신경망에 역으로 넣어 계산해서 최종적으로 입력층까지 계산하는 방법이다. 이를 통해서 가중치를 수작업으로 한땀한땀 아닌 하나의 알고리즘으로써 유의미한 값을 갱신할 수 있게 해주었다. <br>\n",
    "<br>\n",
    "아래는 포유류와 조류를 구분하는 분류기 코드이다. <br>\n",
    "심층 신경망은 다음 시간에 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "[[0, 0],[1, 0],[1, 1],[0, 0],[0, 0],[0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.add(tf.matmul(X, W),b)\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 1은 행 별로 더 하라는 뜻이다.<br>\n",
    "더한다는 것은 차원을 축소한다는 것과 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 경사하강법으로 최적화합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 텐서플로의 세션을 초기화합니다.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 앞서 구성한 특징과 레이블 데이터를 이용해 학습을 100번 진행합니다.\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y:y_data})\n",
    "    \n",
    "    # 학습 도중 10번에 한 번씩 손실값을 출력해봅니다.\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis = 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171211 월\n",
    "p.68~<br>\n",
    "심층 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "[[0, 0],[1, 0],[1, 1],[0, 0],[0, 0],[0, 1]])\n",
    "\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10],-1.,1.))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "L1 = tf.add(tf.matmul(X, W1),b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "model = tf.add(tf.matmul(L1, W2),b2)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 앞서 구성한 특징과 레이블 데이터를 이용해 학습을 100번 진행합니다.\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y:y_data})\n",
    "    \n",
    "    # 학습 도중 10번에 한 번씩 손실값을 출력해봅니다.\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))\n",
    "\n",
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis = 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171212 화\n",
    "\n",
    "# Ch 5. 텐서보드와 모델 재사용\n",
    "\n",
    "(내일 예정)\n",
    "# 20171213 수\n",
    "(이어서 계속)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('./data.csv',delimiter=',',\n",
    "                 unpack=True, dtype='float32')\n",
    "\n",
    "x_data = np.transpose(data[0:2])\n",
    "y_data = np.transpose(data[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used to train, but to count train steps.\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1., 1.))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1))\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([10, 20], -1., 1.))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2))\n",
    "\n",
    "W3 = tf.Variable(tf.random_uniform([20, 3], -1., 1.))\n",
    "model = tf.matmul(L2,W3)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 층 수와 은닉층의 뉴런 수를 늘리면 좀더 복잡한 문제를 푸는데 좋은 성능을 거둘 수 있다.<br>\n",
    "하지만 너무 많이 늘리면 과적합에 빠지기 쉽다. 효과적인 신경망 구축을 위해서는 최적화된 신경망 층 수와 은닉층의 뉴런 수를 설계할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(2):\n",
    "    sess.run(train_op, feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    print('Step: %d' % sess.run(global_step),\n",
    "         'Cost: %.3f' % sess.run(cost, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess, './model/dnn.ckpt',global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print('Prediction:', sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('Target:', sess.run(target, feed_dict={Y:y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('Accuracy: %.2f' % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171221 목\n",
    "Ch 6. Hello 딥러닝 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([784,256], stddev=0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1))\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256], stddev=0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2))\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L2, W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                              feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "    print('Epoch:','%04d'%(epoch+1),\n",
    "         'Avg. cost=', '{:.3f}'.format(total_cost / total_batch))\n",
    "    \n",
    "print('Optimization complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', sess.run(accuracy,\n",
    "                          feed_dict={X: mnist.test.images,\n",
    "                                    Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171222 금\n",
    "드롭아웃 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256], stddev=0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1))\n",
    "L1 = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256], stddev=0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2))\n",
    "L2 = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L2, W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                              feed_dict={X: batch_xs, Y: batch_ys\n",
    "                                        ,keep_prob: 0.8})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "    print('Epoch:','%04d'%(epoch+1),\n",
    "         'Avg. cost=', '{:.3f}'.format(total_cost / total_batch))\n",
    "    \n",
    "print('Optimization complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy,\n",
    "                          feed_dict={X: mnist.test.images,\n",
    "                                    Y: mnist.test.labels,\n",
    "                                    keep_prob:0.8}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256], stddev=0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1))\n",
    "L1 = tf.layers.batch_normalization(L1, training=is_training)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256], stddev=0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2))\n",
    "L2 = tf.layers.batch_normalization(L2, training=is_training)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L2, W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'Avg. cost=', '0.194')\n",
      "('Epoch:', '0002', 'Avg. cost=', '0.086')\n",
      "('Epoch:', '0003', 'Avg. cost=', '0.061')\n",
      "('Epoch:', '0004', 'Avg. cost=', '0.045')\n",
      "('Epoch:', '0005', 'Avg. cost=', '0.039')\n",
      "('Epoch:', '0006', 'Avg. cost=', '0.034')\n",
      "('Epoch:', '0007', 'Avg. cost=', '0.028')\n",
      "('Epoch:', '0008', 'Avg. cost=', '0.026')\n",
      "('Epoch:', '0009', 'Avg. cost=', '0.022')\n",
      "('Epoch:', '0010', 'Avg. cost=', '0.018')\n",
      "('Epoch:', '0011', 'Avg. cost=', '0.020')\n",
      "('Epoch:', '0012', 'Avg. cost=', '0.016')\n",
      "('Epoch:', '0013', 'Avg. cost=', '0.015')\n",
      "('Epoch:', '0014', 'Avg. cost=', '0.013')\n",
      "('Epoch:', '0015', 'Avg. cost=', '0.014')\n",
      "('Epoch:', '0016', 'Avg. cost=', '0.014')\n",
      "('Epoch:', '0017', 'Avg. cost=', '0.012')\n",
      "('Epoch:', '0018', 'Avg. cost=', '0.010')\n",
      "('Epoch:', '0019', 'Avg. cost=', '0.009')\n",
      "('Epoch:', '0020', 'Avg. cost=', '0.010')\n",
      "('Epoch:', '0021', 'Avg. cost=', '0.010')\n",
      "('Epoch:', '0022', 'Avg. cost=', '0.009')\n",
      "('Epoch:', '0023', 'Avg. cost=', '0.011')\n",
      "('Epoch:', '0024', 'Avg. cost=', '0.007')\n",
      "('Epoch:', '0025', 'Avg. cost=', '0.006')\n",
      "('Epoch:', '0026', 'Avg. cost=', '0.008')\n",
      "('Epoch:', '0027', 'Avg. cost=', '0.008')\n",
      "('Epoch:', '0028', 'Avg. cost=', '0.006')\n",
      "('Epoch:', '0029', 'Avg. cost=', '0.006')\n",
      "('Epoch:', '0030', 'Avg. cost=', '0.006')\n",
      "Optimization complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                              feed_dict={X: batch_xs, Y: batch_ys\n",
    "                                        ,is_training: True})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "    print('Epoch:','%04d'%(epoch+1),\n",
    "         'Avg. cost=', '{:.3f}'.format(total_cost / total_batch))\n",
    "    \n",
    "print('Optimization complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.9824)\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy,\n",
    "                          feed_dict={X: mnist.test.images,\n",
    "                                    Y: mnist.test.labels\n",
    "                                    ,is_training: True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
