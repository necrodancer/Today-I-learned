{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3분 딥러닝 3장 실습\n",
    "\n",
    "# 20171130 목\n",
    "p.35~38\n",
    "\n",
    "작성된 소스 코드의 출처 : https://github.com/golbin/TensorFlow-Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant의 반환값은 Tensor()라고 해서 자료형과 속성에 대해 설명한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1., 2., 3.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1., 2., 3.], [4., 5., 6.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 텐서플로에서 수학식 계산을 위한 가장 기본적인 자료형.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랭크 = '\\['의 갯수, 즉 차원 수. \n",
    "(예) 3 : 랭크 0, [3]: 랭크 1, [[3]]: 랭크 2, [[[3]]]: 랭크 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랭크 0부터 순서대로, 스칼라, 벡터, 행렬, ... n차원 텐서라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셰이프는 랭크 별 원소 개수.\n",
    "(예) 3 : 랭크 0 셰이프 \\[\\], \\[3\\] : 랭크 1 셰이프 \\[1\\], [[[1., 2., 3.]],[[1., 2., 3.]]] : 랭크 3 셰이프 [2, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 프로그램 구조 : 1. 그래프 생성(동그라미, tf.constant, tf.add, ...) 2. 그래프 실행(Session run). 이런 실행 방식을 '지연 실행' 또는 'Lazy Evaluation'이라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 실행은 세션 안에서 이루어져야 한다. 아래는 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n",
      "Hello, TensorFlow!\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a,b)\n",
    "print(c)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리 : 텐서플로우는 그래프 기반 머신러닝 라이브러리이다. 프로그램 실행은 그래프 생성, 그래프 실행 순으로 진행된다. 랭크는 \\[의 개수이며 차원 수를 의미한다. 랭크 0부터 스칼라, 벡터, 행렬, ... n차원 텐서라고 부른다. 셰이프는 각 랭크 별 원소 수를 뜻한다. [제일 큰 랭크의 원소수, 그 다음 랭크의 원소 수, ... ]로 적힌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171206 수\n",
    "<br>\n",
    "p.38~43\n",
    "<br>\n",
    "플레이스홀더와 변수\n",
    "플레이스홀더 : 그래프에 입력 값을 담을 그릇(parameter)을 담당.\n",
    "변수(variable) : 학습 함수들이 학습 결과를 갱신하기 위해 사용하는 변수(가중치)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# None : 크기가 정해져 있지 않다.\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,3],[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.random_normal은 정규 분포를 따르는 무작위 값으로 초기화한다는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2,3]행렬 = 행이 2, 열이 3\n",
    "<br>\n",
    "(예) [[1,2,3],[1,2,3]] : [2, 3] 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [1, 2, 3]]\n",
      "=== W ===\n",
      "[[ 0.51939726  0.68138808]\n",
      " [ 1.71464252 -0.77968597]\n",
      " [ 0.19617501 -0.22691396]]\n",
      "=== b ===\n",
      "[[ 0.66313279]\n",
      " [ 0.87909079]]\n",
      "=== expr ===\n",
      "[[ 5.20034027 -0.89559305]\n",
      " [ 5.41629839 -0.67963505]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "placeholder가 session에서 돌아가려면 feed_dict를 통해 입력 데이터를 별도로 넣어야 에러가 나지 않는다.\n",
    "<br>\n",
    "애초에 placeholder는 그릇이니까 내용물을 feed_dict로 담는 건 자연스러운 일이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171207 목\n",
    "\n",
    "p. 44~\n",
    "<br>\n",
    "선형 회귀 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X,Y -> 관계(직선 또는 곡선)\n",
    "<br>\n",
    "X -관계(직선 또는 곡선)-> Y\n",
    "<br>\n",
    "머신러닝은 이렇게 X, Y의 관계를 찾아내서\n",
    "<br>\n",
    "새로운 X를 넣었을 때 Y를 예측하는 것을 기본으로 한다.\n",
    "<br>\n",
    "<br>\n",
    "균등 분포란 주사위를 던지는 확률이 모두 1/6이듯이 사건이 일어날 확률이 모두 동일한 확률 분포를 뜻한다.\n",
    "<br>\n",
    "손실 함수, loss function은 한 쌍의 데이터(x,y)에 대한 손실값을 계산하는 함수이다.\n",
    "<br>\n",
    "손실값이란 예측 값과 실제 값 사이의 차이를 뜻한다. 이 차이를 최소화할 수 있다면 예측도는 최대가 될 것이다.\n",
    "<br>\n",
    "손실값은 각 데이터에 대한 오차를 뜻하고, 비용(Cost)는 전체 데이터에 대한 손실을 뜻한다. 즉 각 손실값 등을 평균하는 것 등으로 구할 수 있다.\n",
    "<br>\n",
    "학습이란 W, b와 같은 변수에 다양한 값을 넣고 계산해서 손실값을 계산하고, 이 손실을 최소화하는 W, b를 구하는 과정이다.\n",
    "<br>\n",
    "손실 함수 중 대표적인 것이 예측 값 - 실제 값이다. 즉 차이, 거리를 뜻한다.\n",
    "<br>\n",
    "최소제곱 함수라고 하여서 이 차이에다가 제곱을 씌운 손실 함수를 아래 코드에 사용했다.\n",
    "<br>\n",
    "그리고 비용을 계산하기 위해 각 손실값들을 모두 더한 후 평균을 내는 reduce_mean을 사용했다. 여기서 reduce란 여러 개를 더해서 한 개의 숫자로 결과가 나오기 때문에 붙인 말로 보인다.\n",
    "<br>\n",
    "그렇다면 어떻게 비용을 최소화해야할까? 최적화 함수란 걸 쓰면 된다. 가장 유명한 게 경사하강법, Gradient Descent이다. 이는 손실함수의 미분값(변화량)을 변수에 반영(학습률을 곱해서 통제) 반영해서 손실값을 줄이는 새로운 변수를 갱신하는 방식을 택한다. 따라서 최적화함수를 많이 작동시키면 그 만큼 변수가 갱신될 것이고, 손실값도 점점 줄어들 것이다. (오버피팅은 일단 논외로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.041261163, array([ 0.78460103], dtype=float32), array([ 0.48052654], dtype=float32))\n",
      "(1, 0.033404078, array([ 0.79342943], dtype=float32), array([ 0.47058082], dtype=float32))\n",
      "(2, 0.031746905, array([ 0.79799628], dtype=float32), array([ 0.45909286], dtype=float32))\n",
      "(3, 0.030238071, array([ 0.80289596], dtype=float32), array([ 0.44807577], dtype=float32))\n",
      "(4, 0.028801749, array([ 0.80762941], dtype=float32), array([ 0.43730223], dtype=float32))\n",
      "(5, 0.027433641, array([ 0.81225443], dtype=float32), array([ 0.42679003], dtype=float32))\n",
      "(6, 0.026130505, array([ 0.81676763], dtype=float32), array([ 0.41653025], dtype=float32))\n",
      "(7, 0.024889275, array([ 0.82117242], dtype=float32), array([ 0.40651715], dtype=float32))\n",
      "(8, 0.023707038, array([ 0.82547134], dtype=float32), array([ 0.39674476], dtype=float32))\n",
      "(9, 0.022580942, array([ 0.82966685], dtype=float32), array([ 0.38720727], dtype=float32))\n",
      "(10, 0.021508316, array([ 0.83376157], dtype=float32), array([ 0.37789908], dtype=float32))\n",
      "(11, 0.020486657, array([ 0.83775783], dtype=float32), array([ 0.36881465], dtype=float32))\n",
      "(12, 0.019513529, array([ 0.841658], dtype=float32), array([ 0.35994858], dtype=float32))\n",
      "(13, 0.018586615, array([ 0.84546447], dtype=float32), array([ 0.35129565], dtype=float32))\n",
      "(14, 0.017703742, array([ 0.84917933], dtype=float32), array([ 0.34285071], dtype=float32))\n",
      "(15, 0.0168628, array([ 0.85280496], dtype=float32), array([ 0.33460882], dtype=float32))\n",
      "(16, 0.016061805, array([ 0.85634345], dtype=float32), array([ 0.32656506], dtype=float32))\n",
      "(17, 0.015298866, array([ 0.85979688], dtype=float32), array([ 0.31871468], dtype=float32))\n",
      "(18, 0.014572165, array([ 0.86316729], dtype=float32), array([ 0.31105301], dtype=float32))\n",
      "(19, 0.013879962, array([ 0.86645663], dtype=float32), array([ 0.30357549], dtype=float32))\n",
      "(20, 0.013220645, array([ 0.86966687], dtype=float32), array([ 0.29627773], dtype=float32))\n",
      "(21, 0.012592648, array([ 0.87280005], dtype=float32), array([ 0.28915542], dtype=float32))\n",
      "(22, 0.011994496, array([ 0.87585783], dtype=float32), array([ 0.28220433], dtype=float32))\n",
      "(23, 0.011424755, array([ 0.87884212], dtype=float32), array([ 0.27542031], dtype=float32))\n",
      "(24, 0.010882056, array([ 0.8817547], dtype=float32), array([ 0.26879939], dtype=float32))\n",
      "(25, 0.010365158, array([ 0.88459724], dtype=float32), array([ 0.26233763], dtype=float32))\n",
      "(26, 0.0098728044, array([ 0.88737142], dtype=float32), array([ 0.25603122], dtype=float32))\n",
      "(27, 0.0094038397, array([ 0.89007896], dtype=float32), array([ 0.24987641], dtype=float32))\n",
      "(28, 0.0089571513, array([ 0.89272135], dtype=float32), array([ 0.24386954], dtype=float32))\n",
      "(29, 0.0085316859, array([ 0.89530027], dtype=float32), array([ 0.2380071], dtype=float32))\n",
      "(30, 0.0081264237, array([ 0.89781719], dtype=float32), array([ 0.23228559], dtype=float32))\n",
      "(31, 0.0077404082, array([ 0.90027356], dtype=float32), array([ 0.22670159], dtype=float32))\n",
      "(32, 0.0073727393, array([ 0.90267098], dtype=float32), array([ 0.22125186], dtype=float32))\n",
      "(33, 0.0070225135, array([ 0.90501058], dtype=float32), array([ 0.21593307], dtype=float32))\n",
      "(34, 0.0066889548, array([ 0.90729415], dtype=float32), array([ 0.21074224], dtype=float32))\n",
      "(35, 0.0063712243, array([ 0.90952271], dtype=float32), array([ 0.20567614], dtype=float32))\n",
      "(36, 0.0060685887, array([ 0.91169775], dtype=float32), array([ 0.20073183], dtype=float32))\n",
      "(37, 0.0057803304, array([ 0.91382051], dtype=float32), array([ 0.19590639], dtype=float32))\n",
      "(38, 0.0055057504, array([ 0.91589212], dtype=float32), array([ 0.1911969], dtype=float32))\n",
      "(39, 0.0052442239, array([ 0.91791403], dtype=float32), array([ 0.18660067], dtype=float32))\n",
      "(40, 0.0049951146, array([ 0.9198873], dtype=float32), array([ 0.18211491], dtype=float32))\n",
      "(41, 0.0047578518, array([ 0.92181319], dtype=float32), array([ 0.17773701], dtype=float32))\n",
      "(42, 0.0045318534, array([ 0.92369276], dtype=float32), array([ 0.17346433], dtype=float32))\n",
      "(43, 0.0043165814, array([ 0.9255271], dtype=float32), array([ 0.16929434], dtype=float32))\n",
      "(44, 0.0041115391, array([ 0.92731738], dtype=float32), array([ 0.16522463], dtype=float32))\n",
      "(45, 0.0039162417, array([ 0.92906469], dtype=float32), array([ 0.16125275], dtype=float32))\n",
      "(46, 0.0037302191, array([ 0.93076986], dtype=float32), array([ 0.15737632], dtype=float32))\n",
      "(47, 0.0035530303, array([ 0.93243414], dtype=float32), array([ 0.15359311], dtype=float32))\n",
      "(48, 0.0033842542, array([ 0.93405837], dtype=float32), array([ 0.14990082], dtype=float32))\n",
      "(49, 0.0032234986, array([ 0.93564355], dtype=float32), array([ 0.14629731], dtype=float32))\n",
      "(50, 0.0030703868, array([ 0.93719071], dtype=float32), array([ 0.14278044], dtype=float32))\n",
      "(51, 0.0029245347, array([ 0.93870056], dtype=float32), array([ 0.13934806], dtype=float32))\n",
      "(52, 0.0027856186, array([ 0.94017416], dtype=float32), array([ 0.13599823], dtype=float32))\n",
      "(53, 0.0026532917, array([ 0.9416123], dtype=float32), array([ 0.13272893], dtype=float32))\n",
      "(54, 0.0025272688, array([ 0.94301593], dtype=float32), array([ 0.12953822], dtype=float32))\n",
      "(55, 0.0024072167, array([ 0.94438577], dtype=float32), array([ 0.12642421], dtype=float32))\n",
      "(56, 0.0022928698, array([ 0.94572264], dtype=float32), array([ 0.12338505], dtype=float32))\n",
      "(57, 0.0021839591, array([ 0.9470275], dtype=float32), array([ 0.12041899], dtype=float32))\n",
      "(58, 0.0020802158, array([ 0.9483009], dtype=float32), array([ 0.11752419], dtype=float32))\n",
      "(59, 0.0019814093, array([ 0.94954371], dtype=float32), array([ 0.11469898], dtype=float32))\n",
      "(60, 0.0018872921, array([ 0.95075667], dtype=float32), array([ 0.11194171], dtype=float32))\n",
      "(61, 0.001797637, array([ 0.95194036], dtype=float32), array([ 0.10925068], dtype=float32))\n",
      "(62, 0.0017122509, array([ 0.95309573], dtype=float32), array([ 0.10662439], dtype=float32))\n",
      "(63, 0.001630921, array([ 0.95422328], dtype=float32), array([ 0.1040612], dtype=float32))\n",
      "(64, 0.0015534497, array([ 0.95532376], dtype=float32), array([ 0.10155965], dtype=float32))\n",
      "(65, 0.001479661, array([ 0.95639771], dtype=float32), array([ 0.0991182], dtype=float32))\n",
      "(66, 0.0014093729, array([ 0.95744592], dtype=float32), array([ 0.09673548], dtype=float32))\n",
      "(67, 0.0013424294, array([ 0.95846885], dtype=float32), array([ 0.09441], dtype=float32))\n",
      "(68, 0.0012786641, array([ 0.95946729], dtype=float32), array([ 0.09214047], dtype=float32))\n",
      "(69, 0.0012179253, array([ 0.96044165], dtype=float32), array([ 0.08992547], dtype=float32))\n",
      "(70, 0.0011600688, array([ 0.96139258], dtype=float32), array([ 0.08776371], dtype=float32))\n",
      "(71, 0.0011049672, array([ 0.96232069], dtype=float32), array([ 0.08565394], dtype=float32))\n",
      "(72, 0.0010524805, array([ 0.9632265], dtype=float32), array([ 0.08359487], dtype=float32))\n",
      "(73, 0.0010024888, array([ 0.96411049], dtype=float32), array([ 0.08158531], dtype=float32))\n",
      "(74, 0.00095486408, array([ 0.96497321], dtype=float32), array([ 0.07962403], dtype=float32))\n",
      "(75, 0.00090951269, array([ 0.96581531], dtype=float32), array([ 0.07770996], dtype=float32))\n",
      "(76, 0.00086630759, array([ 0.96663707], dtype=float32), array([ 0.07584185], dtype=float32))\n",
      "(77, 0.00082515756, array([ 0.96743906], dtype=float32), array([ 0.07401866], dtype=float32))\n",
      "(78, 0.00078596221, array([ 0.96822184], dtype=float32), array([ 0.07223931], dtype=float32))\n",
      "(79, 0.00074862875, array([ 0.96898574], dtype=float32), array([ 0.07050271], dtype=float32))\n",
      "(80, 0.00071306759, array([ 0.96973133], dtype=float32), array([ 0.06880788], dtype=float32))\n",
      "(81, 0.00067919609, array([ 0.97045892], dtype=float32), array([ 0.06715377], dtype=float32))\n",
      "(82, 0.00064693618, array([ 0.97116911], dtype=float32), array([ 0.06553946], dtype=float32))\n",
      "(83, 0.00061620428, array([ 0.97186214], dtype=float32), array([ 0.06396392], dtype=float32))\n",
      "(84, 0.00058693532, array([ 0.97253853], dtype=float32), array([ 0.06242627], dtype=float32))\n",
      "(85, 0.00055905292, array([ 0.97319871], dtype=float32), array([ 0.0609256], dtype=float32))\n",
      "(86, 0.00053250155, array([ 0.97384304], dtype=float32), array([ 0.059461], dtype=float32))\n",
      "(87, 0.00050720602, array([ 0.97447187], dtype=float32), array([ 0.0580316], dtype=float32))\n",
      "(88, 0.00048311288, array([ 0.9750855], dtype=float32), array([ 0.05663653], dtype=float32))\n",
      "(89, 0.00046016459, array([ 0.9756844], dtype=float32), array([ 0.05527502], dtype=float32))\n",
      "(90, 0.00043830517, array([ 0.97626895], dtype=float32), array([ 0.05394626], dtype=float32))\n",
      "(91, 0.00041748854, array([ 0.97683942], dtype=float32), array([ 0.05264942], dtype=float32))\n",
      "(92, 0.00039765515, array([ 0.97739619], dtype=float32), array([ 0.05138377], dtype=float32))\n",
      "(93, 0.00037876816, array([ 0.97793961], dtype=float32), array([ 0.05014854], dtype=float32))\n",
      "(94, 0.00036077379, array([ 0.97846991], dtype=float32), array([ 0.048943], dtype=float32))\n",
      "(95, 0.00034363577, array([ 0.97898746], dtype=float32), array([ 0.04776644], dtype=float32))\n",
      "(96, 0.00032731451, array([ 0.9794926], dtype=float32), array([ 0.04661816], dtype=float32))\n",
      "(97, 0.0003117664, array([ 0.97998559], dtype=float32), array([ 0.04549749], dtype=float32))\n",
      "(98, 0.00029695977, array([ 0.98046678], dtype=float32), array([ 0.04440377], dtype=float32))\n",
      "(99, 0.0002828509, array([ 0.98093629], dtype=float32), array([ 0.04333631], dtype=float32))\n",
      "\n",
      "=== Test ===\n",
      "('X: 5, Y:', array([ 4.9480176], dtype=float32))\n",
      "('X: 2.5 Y:', array([ 2.49567699], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "hypothesis = W * X + b # W와 X까 헁렬이 아니므로 matmul을 사용하지 않음.\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data,\n",
    "                                                           Y: y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "    \n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\",sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5 Y:\",sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171208금\n",
    "p.52~<br>\n",
    "# Ch.4 기본 신경망 구현\n",
    "(이어서 해야 함)\n",
    "# 20171210 일\n",
    "(이어서 계속)\n",
    "p.62 ~<br>\n",
    "인공 신경망은 뉴런의 구조에서 영감을 받은 구조로 가중치+활성화 함수의 연결로 구성되어 있다.<br>\n",
    "가장 좋은 결과를 내기 위해 가중치를 일일이 찾을 수고를 제한된 볼츠만, 각종 활성화 함수, 역전파를 통해 찾을 수 있게 되었고, <br>\n",
    "데이터 수의 기하급수 증가, GPU에 의한 컴퓨팅 파워 증가로 인해 신경망 학습은 딥러닝이라는 이름 아래 최고의 호황을 누리고 있다. <br>\n",
    "역전파는 결과값과 실제값의 오차를 각 신경망에 역으로 넣어 계산해서 최종적으로 입력층까지 계산하는 방법이다. 이를 통해서 가중치를 수작업으로 한땀한땀 아닌 하나의 알고리즘으로써 유의미한 값을 갱신할 수 있게 해주었다. <br>\n",
    "<br>\n",
    "아래는 포유류와 조류를 구분하는 분류기 코드이다. <br>\n",
    "심층 신경망은 다음 시간에 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "[[0, 0],[1, 0],[1, 1],[0, 0],[0, 0],[0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "b = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.add(tf.matmul(X, W),b)\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.nn.softmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 1은 행 별로 더 하라는 뜻이다.<br>\n",
    "더한다는 것은 차원을 축소한다는 것과 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1.1395766)\n",
      "(20, 1.1383139)\n",
      "(30, 1.1370722)\n",
      "(40, 1.1358509)\n",
      "(50, 1.1346496)\n",
      "(60, 1.1334683)\n",
      "(70, 1.1323062)\n",
      "(80, 1.1311632)\n",
      "(90, 1.1300391)\n",
      "(100, 1.1289333)\n"
     ]
    }
   ],
   "source": [
    "# 기본적인 경사하강법으로 최적화합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 텐서플로의 세션을 초기화합니다.\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 앞서 구성한 특징과 레이블 데이터를 이용해 학습을 100번 진행합니다.\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y:y_data})\n",
    "    \n",
    "    # 학습 도중 10번에 한 번씩 손실값을 출력해봅니다.\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\xec\\x98\\x88\\xec\\xb8\\xa1\\xea\\xb0\\x92:', array([0, 0, 0, 0, 0, 1]))\n",
      "('\\xec\\x8b\\xa4\\xec\\xa0\\x9c\\xea\\xb0\\x92:', array([0, 1, 2, 0, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis = 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 50.00\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171211 월\n",
    "p.68~<br>\n",
    "심층 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 0.97937375)\n",
      "(20, 0.79200846)\n",
      "(30, 0.61869645)\n",
      "(40, 0.4685922)\n",
      "(50, 0.35646558)\n",
      "(60, 0.26688138)\n",
      "(70, 0.19452591)\n",
      "(80, 0.13982087)\n",
      "(90, 0.10059301)\n",
      "(100, 0.07326252)\n",
      "('\\xec\\x98\\x88\\xec\\xb8\\xa1\\xea\\xb0\\x92:', array([0, 1, 2, 0, 0, 2]))\n",
      "('\\xec\\x8b\\xa4\\xec\\xa0\\x9c\\xea\\xb0\\x92:', array([0, 1, 2, 0, 0, 2]))\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "[[0, 0],[1, 0],[1, 1],[0, 0],[0, 0],[0, 1]])\n",
    "\n",
    "y_data = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10],-1.,1.))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 3], -1., 1.))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "L1 = tf.add(tf.matmul(X, W1),b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "model = tf.add(tf.matmul(L1, W2),b2)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 앞서 구성한 특징과 레이블 데이터를 이용해 학습을 100번 진행합니다.\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y:y_data})\n",
    "    \n",
    "    # 학습 도중 10번에 한 번씩 손실값을 출력해봅니다.\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(step+1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))\n",
    "\n",
    "prediction = tf.argmax(model, axis = 1)\n",
    "target = tf.argmax(Y, axis = 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X:x_data}))\n",
    "print('실제값:', sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도: %.2f' % sess.run(accuracy * 100, feed_dict = {X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20171212 화\n",
    "\n",
    "# Ch 5. 텐서보드와 모델 재사용\n",
    "\n",
    "(내일 예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
